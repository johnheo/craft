{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import timm\n",
    "from timm.models import create_model\n",
    "mod_list = timm.list_models('*vit_base*')\n",
    "# mod_list\n",
    "\n",
    "#wide_resnet50_2\n",
    "#resnet18\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = create_model('vit_base_patch16_224', pretrained=False, \n",
    "#                      num_classes=100, img_size=32)\n",
    "\n",
    "model = timm.create_model('efficientnet_b0', pretrained=False, num_classes=100)\n",
    "# model.classifier = torch.nn.Linear(1280, 100, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/hdd-nfs/johnheo/wacv/wanda/image_classifiers/ckpts/effnet-B0-sam.pt\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "DIR='/mnt/hdd-nfs/johnheo/wacv/wanda/image_classifiers/ckpts/'\n",
    "model = 'effnet-B0'\n",
    "sam = True\n",
    "if sam:\n",
    "    model += '-sam'\n",
    "else:\n",
    "    model += '-nosam'\n",
    "path = f'{DIR}{model}.pt'\n",
    "ckpt = torch.load(path)\n",
    "print(path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from wanda.image_classifiers.data import load_cifar\n",
    "train_set, dataset_val = load_cifar('cifar100')\n",
    "sampler_val = torch.utils.data.SequentialSampler(dataset_val)\n",
    "data_loader_val = torch.utils.data.DataLoader(\n",
    "    dataset_val, sampler=sampler_val,\n",
    "    batch_size=int(1.5 * 256),\n",
    "    num_workers=3,\n",
    "    pin_memory=True,\n",
    "    drop_last=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([384, 3, 32, 32]) tensor([49, 33, 72, 51, 71, 92, 15, 14, 23,  0, 71, 75, 81, 69, 40, 43, 92, 97,\n",
      "        70, 53, 70, 49, 75, 29, 21, 16, 39,  8,  8, 70, 20, 61, 41, 93, 56, 73,\n",
      "        58, 11, 25, 37, 63, 24, 49, 73, 56, 22, 41, 58, 75, 17,  4,  6,  9, 57,\n",
      "         2, 32, 71, 52, 42, 69, 77, 27, 15, 65,  7, 35, 43, 82, 63, 92, 66, 90,\n",
      "        67, 91, 32, 32, 82, 10, 77, 22, 71, 78, 54,  6, 29, 89, 78, 33, 11, 67,\n",
      "        22, 18, 27, 21, 13, 21, 50, 75, 37, 35, 26, 83, 47, 95, 43, 69, 76, 17,\n",
      "        57, 59, 25, 20, 27,  0,  9, 71,  8, 43, 57, 56, 85, 10, 19, 92, 33, 20,\n",
      "        21, 50, 70, 46, 11, 16,  1, 74, 33, 91, 60, 64, 52, 23,  4, 11, 52, 37,\n",
      "        24, 95, 25, 39, 51, 58, 58, 77, 18, 59, 45, 66, 58, 20, 24,  4, 36,  8,\n",
      "        87, 10, 30, 47, 54, 99, 51, 83,  9, 37,  4, 83, 95, 83, 32, 73, 18, 40,\n",
      "        39, 64, 22, 80, 28, 28, 40, 95, 98, 83, 12, 24, 45, 13, 94, 24, 58, 63,\n",
      "         7, 87,  6, 78, 68, 60,  6, 23, 44, 31, 80, 93, 73, 98, 49, 90, 97, 59,\n",
      "         2, 67, 16, 81, 94, 27, 76, 77, 12, 18,  0, 76, 79, 71, 89, 57, 47, 24,\n",
      "        65,  0, 32, 36, 82, 31, 23, 24, 34, 21, 11, 53, 80, 44,  4, 39, 91, 16,\n",
      "        36, 68, 50, 97, 58, 31,  6, 42, 80, 76, 89, 55, 19, 91, 70,  1,  6, 62,\n",
      "        99, 51, 96, 83, 42, 18, 67, 66, 40, 62, 78, 84, 28, 89, 30, 66, 18, 38,\n",
      "        42, 92, 27, 11, 86, 44, 96, 12, 16, 67, 43, 89, 96, 12, 40,  7, 86, 77,\n",
      "        76, 31, 18, 28, 19, 18, 41, 42, 43, 31, 93, 15, 10,  8, 37, 89, 32, 67,\n",
      "        12,  2, 19, 91, 94,  7, 71, 36, 61, 62,  5, 60, 45, 91, 34, 95,  9, 40,\n",
      "        68, 62, 99, 66, 46,  7, 10, 10, 68, 34, 37, 58, 48, 40, 96, 14, 11, 66,\n",
      "        64, 39, 33, 94, 63, 10, 89, 92, 90, 65, 90, 33, 28, 29, 87, 86,  7,  0,\n",
      "        94, 11, 26,  6, 41, 21])\n"
     ]
    }
   ],
   "source": [
    "for batch in data_loader_val:\n",
    "    images = batch[0]\n",
    "    target = batch[-1]\n",
    "    print(images.shape, target)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "awq",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
